# LLM-Adapter-GLUE-Benchmark

## üìå Overview

This repository benchmarks **eight adapter-based fine-tuning strategies** (using a TinyBERT backbone) across **four GLUE tasks**: SST-2, QNLI, MNLI, and QQP.

### ‚úÖ What This Repo Does:

* Trains the following adapter variants:
  * **Soft Prompt**, **Prefix**, **LoRA**, **Soft Prompt + LoRA**, **Prefix + LoRA**, **(IA)^3**, **Single-Layer Fine-Tuning**, **Full Fine-Tuning**

* Applies the **DP-SGD** optimization algorithm with two privacy budgets:
  * `epsilon = 8` (differentially private training)
  * `epsilon = ‚àû` (standard, non-private training)

* Evaluates **classification accuracy** of each adapter on the corresponding GLUE validation sets.

### üß∞ Tech Stack:

* **Environment**: Google Colab
* **Frameworks**: `PyTorch`, `Transformers`, `Opacus`, `PEFT`, `huggingface_hub`

---

## üöÄ Getting Started

1. Copy the `llm_adapters_comparison/` folder to your Colab environment.

2. Configure hyperparameters via JSON:
   * `hyper_parameter_config_w_privacy.json` (with DP)
   * `hyper_parameter_config_wo_privacy.json` (without DP)

3. Open and run `main.ipynb` step-by-step:
   * Set `project_root_dir` to the path of the copied folder.
   * Navigate (`cd`) to `project_root_dir`.
   * Generate a Hugging Face access token and store it in Colab secrets.

4. Open the .xlsx files to check the results.
   * The final results are saved in the `output/` folder:
      - `accuracy_results_eps_*.xlsx` saves classification accuracy
      - `parameter_scale_eps_*.xlsx` saves the number of trainable parameters from the adapters

---

## üè† Run Locally with uv

- Sync dependencies once: `uv sync`
- (Optional) export a Hugging Face token so the scripts can download gated models: `export HF_TOKEN=<your-token>`
- Launch the Python entry point (recreates the notebook flow without Colab): `uv run python llm_adapters_comparison_local/main.py --config wo_privacy`
- Add `--force-download` the first time to cache GLUE datasets locally, and switch `--config` between `wo_privacy`, `w_privacy`, or `both` as needed.

